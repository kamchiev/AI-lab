{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Y2ohbbqDrItVGiUOHzquJ7owC130h1Oi","timestamp":1724765292715}],"authorship_tag":"ABX9TyNYxYrJ717/OERVdaeKJ17K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#convolurions only helps to extract the features but they do not classify\n","#to classufy images in CNN, we simply put Fully Connected Layer at the end (perceptron)\n","#Pooling layer - not necessary to use. it reduce the siz of image.\n","#Fully connected layer - we dont flatten the image instead we flatten the features that we got at the end of last cnn layer and pass it to mlp\n","#training adjust weight and values inside the convalutions\n","#at each levels convalutions are applied parallel but at each level, convolutions are applied subsequntly (result of 1st layer goes to 2nd etc)\n","# stride - how much we want to move our filer(by default it is 1)"],"metadata":{"id":"0b-WFuVuvwh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HTLxnwoUKhYX","executionInfo":{"status":"ok","timestamp":1725531821445,"user_tz":-120,"elapsed":5610,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.image as img\n","from torch import nn   #stands for neural networks\n","import matplotlib.pyplot as plt\n","import torchmetrics"]},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"XZnlCVdIEFqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data = datasets.FashionMNIST(root=\"data\",train=True,download=True, transform=ToTensor())\n","test_data = datasets.FashionMNIST(root=\"data\",train=False,download=True, transform=ToTensor())"],"metadata":{"id":"KmBIUDtKwxIB","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":13407,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"634d43a9-4b24-4dca-e3d5-8d21423eb672"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:08<00:00, 3233415.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 198540.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:01<00:00, 3697745.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 15184442.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["print(training_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NsMZpYk8EQ2V","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":9,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}},"outputId":"71c06cf7-6ea8-485d-8daf-e76dc7e400f4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset FashionMNIST\n","    Number of datapoints: 60000\n","    Root location: data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()\n"]}]},{"cell_type":"code","source":["# create the model\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"M2ieoe4cK_j9","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":7,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**Sequencial** part helps to execute things sequencially such that output of the firt layer goes to the input of the second layer. Inside Sequential, we must define all the layers.\n","  When weights togther with the data reaches the threshold, neuron fires,\n","  we set this threshold with **activation funtion**. (Sigmoid is an example)\n","\n","Then, we do the **forward pass** which defines how data goes from input later to output layer.\n","\n"],"metadata":{"id":"Vf9yLb6tJ2Mi"}},{"cell_type":"code","source":["class OurCNN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.cnn = nn.Sequential(\n","\n","\n","      nn.Conv2d(3,5,3),   #1st layer is input layer, 1 gray scale image has only one channel, 5 is number of filters, size of kernel # (64,1,28,28)\n","      #signoid can be used but it is quite old throug\n","      nn.ReLU(), #we can use relu (rectified linear unit), activation function. it computer max value between 0 and layer above\n","      nn.Conv2d(5,10,3), #(64,5,26,26) d-k+1 (dimention - kernel size)\n","      nn.ReLU(),\n","      nn.MaxPool2d(2)\n","    )\n","\n","\n","    self.mlp = nn.Sequential(\n","      #we want use extracted features to classify. as we said we use perceptron\n","\n","      nn.Linear(12*12*10,10), #we can put any number instead 2 bc result of flatten layer will be put as input\n","      nn.ReLU(),\n","      nn.Linear(10,10) #1st 10 - output size of above level, 2nd 10 number of classes\n","\n","      )\n","\n","\n","\n","       #layer\n","\n","  def forward(self,x):\n","\n","      x = self.cnn(x)\n","      x = torch.flatten(x,1) #reducing the size to 1 to do the classification\n","      x = self.mlp(x)\n","      return x\n","# shape of the image\n","# (B,C,W,H)\n","\n","#input size in mlp should be equal to the number of pixels of the image, while cnn does not care about that\n","#FCN uses CNN even in last layer instead of MLP. we use it for segmentation"],"metadata":{"id":"N3OHWwT3LBgs","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":7,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# HERE WE WANT TO IMPLEMENT THE MODEL WITHOUT SEQUENTIAL\n","class OurCNN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","\n","    self.input_layer = nn.Conv2d(1,5,3)\n","\n","    self.relu = nn.ReLU()\n","    self.conv_1 = nn.Conv2d(5,10,3)\n","\n","    self.max_pool = nn.MaxPool2d(2) #take max from 2 by 2 kernel\n","\n","\n","    self.input_linear = nn.Linear(12*12*10,10)\n","\n","    self.output_linear = nn.Linear(10,10)\n","\n","\n","  def forward(self,x):\n","      x = self.input_layer(x)\n","      x = self.relu(x)\n","      x = self.conv_1(x)\n","      x = self.relu(x)\n","      x = self.max_pool(x)\n","\n","\n","      x = torch.flatten(x,1)\n","\n","      x = self.input_linear(x)\n","      x = self.relu(x)\n","      x = self.output_linear(x)\n","\n","      return x\n"],"metadata":{"id":"f0I6tkCI2Ysm","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":7,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model = OurCNN().to(device)"],"metadata":{"id":"PITZSHx0EL9y","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":7,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# define hyperparameters\n","epochs = 2 # how many times our model analyse the data.\n","batch_size = 64\n","learning_rate = 0.001 # amount of the weight change in each iteration. lower rate means, better training however it takes more time"],"metadata":{"id":"ulFiKq9pLfGv","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#create the dataloader\n","train_dataloader = DataLoader(training_data,batch_size=batch_size)\n","test_dataloader = DataLoader(test_data,batch_size=batch_size)"],"metadata":{"id":"aRGXsJaIpPis","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# define the loss function\n","loss_fn = nn.CrossEntropyLoss() # we may use any loss functions. it takes arrays of logits and turns into array of prob.\n","#to compute the error\n","#define optimizer\n","optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n","# we may change SGD to AdamW"],"metadata":{"id":"vMhgeiwzLWRe","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# create the accucary metric\n","metric = torchmetrics.Accuracy(task=\"multiclass\",num_classes=10)\n"],"metadata":{"id":"fmEqjjgstUtS","executionInfo":{"status":"ok","timestamp":1725531837742,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#define the training loop\n","def train_loop(dataloader, model,loss_fn,optimizer):\n","\n","  #get the batch from the dataset\n","  for batch,(X,y) in enumerate(dataloader):\n","    #compute prediction and loss\n","    pred = model(X)\n","    #y_tensor = torch.tensor([y])\n","    loss = loss_fn(pred,y)  #since we are passing 64 elements of tensor, convertion is not necessary. earlier we were passing only 1 data at a time\n","\n","    #backpropogation\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    #print the loss during the training, we do not need to use the below code to print there exists a package to do it\n","    if batch % 100 == 0:\n","      acc = metric(pred,y)\n","      print(f\"accuracy current batch: {acc}\")\n","\n","  acc = metric.compute()\n","  print(f\"training final accuracy for each epoch : {acc}\")\n","  metric.reset() #reset the accuracy\n","\n","\n"],"metadata":{"id":"5xjVqLMFSFSQ","executionInfo":{"status":"ok","timestamp":1725531837743,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#test loop\n","def test_loop(dataloader, model, loss_fn):\n","  #we dont use optimizer bc it is already optimized in training stage, optimizer serves to change the weight\n","  size = len(dataloader)\n","  num_batches = len(dataloader)\n","  test_loss, correct = 0,0\n","  #disable weight update bc we are testing\n","\n","  with torch.no_grad():\n","    for X,y in dataloader:\n","      pred = model(X)\n","      acc = metric(pred,y)\n","  acc = metric.compute()\n","  print(f\"testing final accuracy for each epoch : {acc}\")\n","  metric.reset() #reset the accuracy\n"],"metadata":{"id":"4QLL4ObVLXtv","executionInfo":{"status":"ok","timestamp":1725531837743,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["#train the model\n","for t in range(epochs):\n","  print(f\"epochs: {t}\")\n","  train_loop(train_dataloader,model, loss_fn,optimizer)\n","  test_loop(test_dataloader,model,loss_fn)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaYW_ZwtRttg","executionInfo":{"status":"ok","timestamp":1725531893705,"user_tz":-120,"elapsed":55968,"user":{"displayName":"Hayotbek Kamchiev","userId":"05791516441869419679"}},"outputId":"2068dc47-998d-4374-da45-edc618f42b25"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["epochs: 0\n","accuracy current batch: 0.046875\n","accuracy current batch: 0.390625\n","accuracy current batch: 0.8125\n","accuracy current batch: 0.65625\n","accuracy current batch: 0.65625\n","accuracy current batch: 0.75\n","accuracy current batch: 0.734375\n","accuracy current batch: 0.765625\n","accuracy current batch: 0.671875\n","accuracy current batch: 0.796875\n","training final accuracy for each epoch : 0.628125011920929\n","testing final accuracy for each epoch : 0.7734000086784363\n","epochs: 1\n","accuracy current batch: 0.828125\n","accuracy current batch: 0.765625\n","accuracy current batch: 0.859375\n","accuracy current batch: 0.796875\n","accuracy current batch: 0.71875\n","accuracy current batch: 0.828125\n","accuracy current batch: 0.828125\n","accuracy current batch: 0.8125\n","accuracy current batch: 0.765625\n","accuracy current batch: 0.828125\n","training final accuracy for each epoch : 0.8031250238418579\n","testing final accuracy for each epoch : 0.800599992275238\n"]}]},{"cell_type":"code","source":["# since perceptron requires flattening the dimnetion to 1. and we loose information. it makes mlp not well for images"],"metadata":{"id":"-f7Y31d93gkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#to optimize, we must play with the hyperparameters manually"],"metadata":{"id":"JRYl0vlG1TW_"},"execution_count":null,"outputs":[]}]}